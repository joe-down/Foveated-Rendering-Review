\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{svg}
\addbibresource{assets/bib.bib}

\title{What are the main hurdles to the adoption of foveated rendering in HMDs?}
\author{Joe Down}
\date{April 11, 2024}

\begin{document}

\maketitle
%\tableofcontents

\section{Background}
Humans direct their gaze towards an object by rotating their eyes such that light reflected by the object viewed falls on the eye's fovea\cite{levoy1990gaze}. This is a small region at the centre of the retina with increased cone density\cite{pumphrey1948theory}: the photoreceptor cells of the eye which send signals to brain when stimulated by light\cite{arendt2003evolution}. Additionally, the brain is structured to process this information with a space-variant resolution dedicating most resources to the fovea region\cite{weber2009implementations}. These conditions mean that spatial acuity is strongest for objects viewed within this area\cite{levoy1990gaze}.

In virtual reality systems, it has been found that delay in frame delivery results in motion sickness, discomfort\cite{waltemate2016impact}, and decreased performance in certain motor tasks\cite{raaen2015measuring}. Since rendering requirements continue to increase as screen reolutions and scene complexities increase, it remains difficult for modern computing devices to render complex VR scenes with low visual latency\cite{wang2023foveated}. Foveated rendering has frequently been proposed as a technique to reduce frame render times and help mitigate this problem. It aims to, in one of a few ways, reduce render quality for parts of a VR scene in the periphery and/or increase render quality within the foveal view\cite{weier2016foveated}, exploiting the eye's space-variant resolution. \textcite{mohanto2022integrative} specify a range of subcategories for render quality reductions, all falling within 4 main subcategories: adaptive resolution (i.e. render the periphery with lower resolution or some other form of reduced sampling), geometric simplification (i.e. render objects with a less detailed model in the periphery), shading simplification/chromatic degeneration (i.e. simplify lighting simulation for pixels in the periphery), and spatio-temporal deterioration (i.e. partially reusing parts of the frame in the periphery to avoid having to re-render the entire view at each refresh). A simplified breakdown of these categories is shown in \cref{fig:types}.

\begin{figure}
  \begin{center}
    \includesvg[width=\textwidth]{assets/types}
    \caption{\citeauthor{mohanto2022integrative}'s\cite{mohanto2022integrative} foveated rendering subcategories.}
    \label{fig:types}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includesvg[width=\textwidth]{assets/pipeline}
    \caption{High level foveated rendering pipeline.}
    \label{fig:pipeline}
  \end{center}
\end{figure}

To understand existing limitations, a high-level structure of a foveated rendering pipeline needs to be defined. There are two main types of foveated rendering pipeline with slightly different structures: static, and dynamic. Dynamic foveated rendering requires some form of eye tracking capability within the HMD to determine which part of the stereoscopic displays the corresponding eye is directed towards. In software, frames must be received from this eye tracking device, and processed using some algorithm to determine where the eye is directed within a scene and/or on the physical display. The program being used must then be able to take this information and use it to in some way change how the scene is rendered. The rendered image must then be transmitted back to the headset and displayed, all within a short enough timeframe to avoid noticeable visual latency. Static foveated rendering is mostly the same regarding the software implementation, however rather than requiring eye tracking hardware and processing, it simply assumes the fovea region is in the centre of each display and performs the rendering optimisations accordingly. This is not a realistic model, but will lead to performance gains at the cost of likely noticeable quality loss when the user averts their gaze from the centre. A summary of this pipeline can be viewed in \cref{fig:pipeline}.

Static foveated rendering has been used by certain applications with non specialised hardware for years now. Dynamic foveated rendering has only started to become more common within the past few years, as increasingly mainstream HMDs such as PSVR2\cite{vr2_fove} and Apple Vision Pro\cite{vision_pro_announce} have been released with eye tracking functionality. The following sections aim to address why dynamic foveated rendering adoption is not yet ubiquitous across all HMDs, and identify shortcomings with existing implementations of both types of foveated rendering. First I will address the hardware and software latency requirements, followed by other software requirements, hardware requirements, and usability requirements.

\section{Time Limitations}
Most limitations relate to the time taken to perform all of the foveated rendering process and optimisations. If this cannot be done in a shorter amount of time than just rendering an unchanged frame, then the entire process is redundant as visual latency has been worsened. Additionally, with dynamic foveated rendering, if the foveated region cannot be updated with low enough latency, the issue shown in \cref{fig:eye} will occur where the user noticeably looks into the lower quality region of the scene after large, rapid eye movements (saccades). \textcite{albert2017latency} suggest gaze tracking latency for foveation of any level is unnoticeable with 20-40ms latency, becomes difficult to notice with less than 50-70ms of latency, while less aggressive foveation could be tolerated with latencies up to 150ms. \textcite{li2020optical} similarly suggest an upper bound of 50ms latecy, while no paper on the topic seems to aim for a latency below 14ms\cite{koskela2018instantaneous}. In this section, issues will be broken down in the order of the dynamic foveated rendering pipeline as defined in \cref{fig:pipeline}. Difficulties faced will be similar for static foveated rendering, ignoring parts relating to eye tracking.
\subsection{Eye Tracking Sensor}
The first time-based (hardware) consideration is the time taken to acquire sensor data for eye tracking. \textcite{stein2021comparison} performed experiments to determine eye tracking delays for three headsets with eye tracking capabilities: FOVE-0, Varjo VR-1, and the HTC Vive Pro Eye. They found that the delay before the eye movement data became available ranged from between 15-52ms. Given the previously described latency requirements for unnoticeable gaze tracking, this leaves little time to process, render, and display within the 20-40ms available. It does leave some room however for less aggressive foveation possible with 50-70ms latency. This suggests that perhaps foveated rendering has not yet become ubiquitous due to current hardware latency difficulties in existing, expensive, headsets.

\textcite{stein2021comparison} also test the EyeLink 1000 eye tracker, an external webcam-like device capable of sampling at 1000HZ. From the same experimental setup this had a measurable delay of 0ms, suggesting significant room for improvement if this technology can be miniaturised. I was unable to find gaze delay data for newer headsets so better results may already be present, but these headsets remain expensive.

\subsection{Gaze Processing}
The second time-based (software) consideration is regarding the processing of eye tracking data from a sensor to determine gaze position. This is often performed using a trained machine learning model. TODO FINISH

\subsection{Scene Rendering Optimisation}
The third time-based (based) consideration is regarding the gaze optimised rendering of the scene. TODO FINISH THIS BIT

\subsection{Displaying in Headset}
The final time-based (based) consideration is regarding displaying the image in the headset. Since this must be performed even when not performing foveated rendering it presumably is not a source of problems. I was unable to find any literature referencing foveated rendering specific difficulties with this step. WHAT IS A NORMAL LATENCY

\begin{figure}
  \begin{center}
    \includesvg[width=\textwidth]{assets/eyes}
    \caption{Large eye movements (saccades) result in the eye looking into the lower quality region before the frame updates. If eye tracking or image display is of high latency, this will be noticeable. Based on a similar diagram by \textcite{albert2017latency}.}
    \label{fig:eye}
  \end{center}
\end{figure}

\section{Other Software Limitations}
\subsection{Implementation Standards}
Another potential issue is with conflicting implementation standards for gaze tracking. OpenXR provides extensions for gaze interaction extensions \cite{xr_gaze}, which is implemented by the Unity engine in the XR Interaction Toolkit \cite{xr_gaze_unity} and Unreal engine through the OpenXREyeTracker plugin\cite{xr_gaze_unreal}. This would seem to imply some sort of standardisation, however I was unable to find clear documentation for how these plugins can be used for foveated rendering. Unity provides the FoveatedRenderingMode render flag\cite{unity_fove}, however this is not well documented and I couldn't determine whether this is dynamic or static foveated rendering. Vendor specific dynamic foveated rendering Unity implementations seem to exist for Meta Quest Pro\cite{meta_unity} and Vive Pro Eye\cite{vive_unity}, but these will work only for those specific headsets. Additionally, not all headsets support the OpenXR standard at all such as Apple Vision Pro and PSVR\cite{openxr_conform}. All of these obstacles would seem to make implementation of foveated rendering for a cross-platform application a significant undertaking.

\subsection{Gaze Tracking Implementation}

\subsection{Scene Rendering Optimisation Implementation}

\section{Other Hardware Limitations}
\subsection{Adoption}
A significant hardware limitation is simply adoption. Since only very few headsets currently implement eye tracking of any sort, there is little motivation for developers to implement foveated rendering support if there is not a significant enough user base. For headsets using SteamVR, the March 2024 Steam hardware survey indicates the most common headset with eye tracking capabilities is the Meta Quest Pro, used by only 0.44\% of VR users. The list of more common headsets accounts for 97.37\% of users. This implies that no more than 2.64\% of users have headsets with eye tracking; certainly less since many headsets ranked lower on the list lack the capability.

\subsection{Cost}
Another large hardware limitation is the cost of eye tracking hardware. I struggled to find any cost breakdowns for the required hardware. An eye tracker add-on for the Vive Focus 3 is sold for \$250\cite{eye_cost_vive}, while costs for a selection of headsets with eye tracking (and further ``premium'' features, meaning the price is not solely indicative of the cost of eye tracking) are broken down in \cref{tab:costs}. These factors would seem to imply costs remain quite high for this feature, likely out of reach for the \$300\cite{quest_two_price} market of a product such as the Meta Quest 2 (used by 37.84\% of SteamVR users, the largest share for any headset).
\begin{table}
  \begin{center}
    \begin{tabular}{|c|c|c|}
      \hline
      Headset & Cost (USD) & Cost Source\\
      \hline
      Vive Focus 3 Add-On & \$250 (+\$1300 for headset) & \cite{eye_cost_vive, focus_price}\\
      \hline
       PSVR2 & \$550 & \cite{psvr_price}\\
       \hline
       Meta Quest Pro & \$1500 & \cite{quest_pro_price}\\
       \hline
       Vive Pro Eye & \$1600 & \cite{vive_pro_eye}\\
       \hline
       Apple Vision Pro & \$3500 & \cite{vision_pro_price}\\
       \hline
       Varjo XR-4 & \$3990 & \cite{varjo_price}\\
       \hline
    \end{tabular}
  \end{center}
  \caption{Costs for current eye tracking enabled VR hardware.}
  \label{tab:costs}
\end{table}

\subsection{Eye Tracking Accuracy and Precision}
\textcite{schuetz2022eye} performed an investigation into the eye tracking spatial accuracy and precision of the Vive Pro Eye headset. They found through their procedure that it exhibited a mean accuracy of 1.08$^{\circ}$, outside the 0.5$^{\circ}$ they claim is typically required to accuratelt identify a user's fixation location. However on a per-user basis, some individuals (usually those without glasses) saw individual mean gaze errors as low as 0.58$^{\circ}$; close to the required threshold. These indicate that modern, high end, consumer VR eye tracking is nearing the required accuracy levels for most eye tracking, however is not yet ready for very small and rapid movements such as micro-saccades. \textcite{schuetz2022eye} therefore suggest that scenes in foveated applications should be designed with gaze targets large enough that these very small eye movements are avoided.

\section{User Perception}
\subsection{Noticeable Foveation}
\textcite{swafford2016user} performed a trial to determine conditions under which foveated rendering could be reliably distinguished from a fully rendered frame. They investigated for three foveated rendering techniques: variable resolution (adaptive resolution in \cref{fig:types}), ambient occlusion (shading simpltification in \cref{fig:types}), and tessellation(geometric simplification in \cref{fig:types}). They found that for all techniques, when peripheral quality decreases were minor, it was very difficult for a user to identify. More users were able to correctly detect the lower quality image when values were more extreme. Even with strong foveation, it was found that diminished peripheral resolution was difficult to detect. Users found distinguishing ambient occlusion and tessellation significantly easier as peripheral quality was diminished. These findings imply that resolution can be greatly diminished for larger performance uplifts, while shading and model quality must be more cautiously tuned to avoid noticeable quality loss.

\subsection{Accessibility}
In a previously mentioned experiment by \textcite{schuetz2022eye} on eye tracking accuracy with the Vive Pro Eye, the researchers found that lower accuracy and precision was yielded for users with glasses (though not with contact lenses). While not identifying a precise reason, the researchers suggested optical factors from the glasses' lenses and obstructions by the glasses' frames as possibilities. While this increased error was not enough to indicate any significant usability impedance, this potential accessibility concern could be relevant in future system designs. Since many people wear glasses, issues regarding this could possibly block increased adoption.

\section{Conclusion}

\printbibliography

\end{document}
